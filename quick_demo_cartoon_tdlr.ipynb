{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a6d85be11f2a40318297da84b4774f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "bluehead.jpg",
              "bluehead_anno.jpg",
              "boy.jpg",
              "cartoonM - Copy.png",
              "cartoonM.png",
              "cartoonM_anno.png",
              "color.jpg",
              "color_anno.jpg",
              "cuphead.png",
              "cuphead_anno.png",
              "danbooru1.jpg",
              "danbooru1_anno.jpg",
              "dog.jpg",
              "dog_anno.jpg",
              "draw.png",
              "draw_anno.png",
              "foa_example.png",
              "girl.png",
              "girl_anno.png",
              "girl_hair.png",
              "johnback.jpg",
              "johnhead.jpg",
              "johnhead.png",
              "johnhead_anno.jpg",
              "monk.jpg",
              "monk.png",
              "napkin.png",
              "napkin_anno.jpg",
              "obama.jpg",
              "obama.png",
              "obama_anno.png",
              "onepunch.png",
              "onepunch_onlybody.jpg",
              "outlet.jpg",
              "outlet.png",
              "outlet_anno.jpg",
              "sketch.png",
              "smiling_person.png",
              "smiling_person_anno.png",
              "smiling_person_example.png",
              "smiling_person_full.png",
              "vangogh.jpg",
              "vangogh_anno.jpg",
              "vangogh_black.jpg",
              "whiteface - Copy.jpg",
              "whiteface.jpg",
              "whiteface_anno.jpg",
              "wilk.png",
              "wilk2_anno - Copy.jpg",
              "wilk2_anno.jpg",
              "wilk_anno.png",
              "wilk_fullbody.jpg",
              "wilksolid.png"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "index": 47,
            "layout": "IPY_MODEL_fe4f94fbb0b14d47a028573f44f58ac3",
            "style": "IPY_MODEL_0d593a520ab546568de139fe021e1f46"
          }
        },
        "fe4f94fbb0b14d47a028573f44f58ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d593a520ab546568de139fe021e1f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FpIfnReur6z"
      },
      "source": [
        "# MakeItTalk Cartoon Quick Demo\n",
        "\n",
        "## TDLR version\n",
        "\n",
        "Remember to change to GPU runtime first!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PZQxNvvuuNZ"
      },
      "source": [
        "## Preparation (run only once)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56UHwyJKuaWw",
        "outputId": "ca5756ab-5bec-4ac4-aaaf-172004f4479b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Git clone project and install requirements...')\n",
        "!git clone https://github.com/yzhou359/MakeItTalk &> /dev/null\n",
        "%cd /content/MakeItTalk\n",
        "!export PYTHONPATH=/content/MakeItTalk:$PYTHONPATH\n",
        "!pip install -r requirements.txt &> /dev/null\n",
        "!pip install tensorboardX &> /dev/null\n",
        "!mkdir examples/dump\n",
        "!mkdir examples/ckpt\n",
        "!pip install gdown &> /dev/null\n",
        "print('Done!')\n",
        "print('Download pre-trained models...')\n",
        "!gdown -O examples/ckpt/ckpt_autovc.pth https://drive.google.com/uc?id=1ZiwPp_h62LtjU0DwpelLUoodKPR85K7x\n",
        "!gdown -O examples/ckpt/ckpt_content_branch.pth https://drive.google.com/uc?id=1r3bfEvTVl6pCNw5xwUhEglwDHjWtAqQp\n",
        "!gdown -O examples/ckpt/ckpt_speaker_branch.pth https://drive.google.com/uc?id=1rV0jkyDqPW-aDJcj7xSO6Zt1zSXqn1mu\n",
        "!gdown -O examples/ckpt/ckpt_116_i2i_comb.pth https://drive.google.com/uc?id=1i2LJXKp-yWKIEEgJ7C6cE3_2NirfY_0a\n",
        "!gdown -O examples/dump/emb.pickle https://drive.google.com/uc?id=18-0CYl5E6ungS3H4rRSHjfYvvm-WwjTI\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlEgsX1Ivzbm"
      },
      "source": [
        "## Animate your photos here!\n",
        "\n",
        "- Upload your images to `examples_cartoon` with size `256x256`. Or use existing ones.\n",
        "\n",
        "- Upload your speech audio files (could be many) to `examples`. Since it will process all `.wav` files under `examples`, remember to delete non-necessary `.wav` files. Or use an existing one `M6_04_16k.wav`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1IVS6EpHhK6"
      },
      "source": [
        "## Step 1/3: Choose your image (in below Dropdown). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe8DGOmwuqlx",
        "outputId": "53bbb1e0-cb83-41e9-916f-1788e01ffdcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297,
          "referenced_widgets": [
            "a6d85be11f2a40318297da84b4774f28",
            "fe4f94fbb0b14d47a028573f44f58ac3",
            "0d593a520ab546568de139fe021e1f46"
          ]
        }
      },
      "source": [
        "import ipywidgets as widgets\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Choose the image name to animate: (saved in folder 'examples_cartoon/')\")\n",
        "img_list = glob.glob1('examples_cartoon', '*.jpg') + glob.glob1('examples_cartoon', '*.png')\n",
        "img_list.sort()\n",
        "img_list = [item for item in img_list if '_bg' not in item]\n",
        "default_head_name = widgets.Dropdown(options=img_list, value='mimi.png')\n",
        "def on_change(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        plt.imshow(plt.imread('examples_cartoon/{}'.format(default_head_name.value)))\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "default_head_name.observe(on_change)\n",
        "display(default_head_name)\n",
        "plt.imshow(plt.imread('examples_cartoon/{}'.format(default_head_name.value)))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bF2RPbCEM2I"
      },
      "source": [
        "## Step 2/3: Setup your animation controllers (on right Sliders)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzeK4TPzy_82"
      },
      "source": [
        "#@markdown # Animation Controllers\n",
        "#@markdown Amplify the lip motion in horizontal direction\n",
        "AMP_LIP_SHAPE_X = 2 #@param {type:\"slider\", min:0.5, max:5.0, step:0.1}\n",
        "\n",
        "#@markdown Amplify the lip motion in vertical direction\n",
        "AMP_LIP_SHAPE_Y = 2 #@param {type:\"slider\", min:0.5, max:5.0, step:0.1}\n",
        "\n",
        "#@markdown Amplify the head pose motion (usually smaller than 1.0, put it to 0. for a static head pose)\n",
        "AMP_HEAD_POSE_MOTION = 0.35 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "\n",
        "#@markdown Add naive eye blink\n",
        "ADD_NAIVE_EYE = True  #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "#@markdown If your image has an opened mouth, put this as True, else False\n",
        "CLOSE_INPUT_FACE_MOUTH = False  #@param [\"False\", \"True\"] {type:\"raw\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0KE1rLxB_Ce"
      },
      "source": [
        "## Step 3/3: One-click to Run (just wait in seconds)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFQh-tlKzQCm",
        "outputId": "8ed40990-fe3c-4f78-bab8-df870aa57535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('thirdparty/AdaptiveWingLoss')\n",
        "import os, glob\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "from src.autovc.AutoVC_mel_Convertor_retrain_version import AutoVC_mel_Convertor\n",
        "import shutil\n",
        "\n",
        "GEN_AUDIO = True\n",
        "GEN_FLS = True\n",
        "\n",
        "%cd /content/MakeItTalk\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--jpg', type=str, default=default_head_name.value)\n",
        "parser.add_argument('--jpg_bg', type=str, default='mimi_bg.jpg')\n",
        "parser.add_argument('--out', type=str, default='out.mp4')\n",
        "\n",
        "parser.add_argument('--load_AUTOVC_name', type=str, default='examples/ckpt/ckpt_autovc.pth')\n",
        "parser.add_argument('--load_a2l_G_name', type=str, default='examples/ckpt/ckpt_speaker_branch.pth') #ckpt_audio2landmark_g.pth') #\n",
        "parser.add_argument('--load_a2l_C_name', type=str, default='examples/ckpt/ckpt_content_branch.pth') #ckpt_audio2landmark_c.pth')\n",
        "parser.add_argument('--load_G_name', type=str, default='examples/ckpt/ckpt_116_i2i_comb.pth') #ckpt_i2i_finetune_150.pth') #ckpt_image2image.pth') #\n",
        "\n",
        "parser.add_argument('--amp_lip_x', type=float, AMP_LIP_SHAPE_X)\n",
        "parser.add_argument('--amp_lip_y', type=float, AMP_LIP_SHAPE_Y)\n",
        "parser.add_argument('--amp_pos', type=float, default=AMP_HEAD_POSE_MOTION)\n",
        "parser.add_argument('--reuse_train_emb_list', type=str, nargs='+', default=[]) #  ['E_kmpT-EfOg']) #  ['E_kmpT-EfOg']) # ['45hn7-LXDX8'])\n",
        "\n",
        "\n",
        "parser.add_argument('--add_audio_in', default=False, action='store_true')\n",
        "parser.add_argument('--comb_fan_awing', default=False, action='store_true')\n",
        "parser.add_argument('--output_folder', type=str, default='examples_cartoon')\n",
        "\n",
        "#### NEW POSE MODEL\n",
        "parser.add_argument('--test_end2end', default=True, action='store_true')\n",
        "parser.add_argument('--dump_dir', type=str, default='', help='')\n",
        "parser.add_argument('--pos_dim', default=7, type=int)\n",
        "parser.add_argument('--use_prior_net', default=True, action='store_true')\n",
        "parser.add_argument('--transformer_d_model', default=32, type=int)\n",
        "parser.add_argument('--transformer_N', default=2, type=int)\n",
        "parser.add_argument('--transformer_heads', default=2, type=int)\n",
        "parser.add_argument('--spk_emb_enc_size', default=16, type=int)\n",
        "parser.add_argument('--init_content_encoder', type=str, default='')\n",
        "parser.add_argument('--lr', type=float, default=1e-3, help='learning rate')\n",
        "parser.add_argument('--reg_lr', type=float, default=1e-6, help='weight decay')\n",
        "parser.add_argument('--write', default=False, action='store_true')\n",
        "parser.add_argument('--segment_batch_size', type=int, default=512, help='batch size')\n",
        "parser.add_argument('--emb_coef', default=3.0, type=float)\n",
        "parser.add_argument('--lambda_laplacian_smooth_loss', default=1.0, type=float)\n",
        "parser.add_argument('--use_11spk_only', default=False, action='store_true')\n",
        "parser.add_argument('-f')\n",
        "\n",
        "opt_parser = parser.parse_args()\n",
        "\n",
        "DEMO_CH = opt_parser.jpg.split('.')[0]\n",
        "\n",
        "shape_3d = np.loadtxt('{}/{}_face_close_mouth.txt'.format(opt_parser.output_folder, DEMO_CH))\n",
        "\n",
        "''' STEP 3: Generate audio data as input to audio branch '''\n",
        "au_data = []\n",
        "au_emb = []\n",
        "ains = glob.glob1('examples', '*.wav')\n",
        "ains = [item for item in ains if item is not 'tmp.wav']\n",
        "ains.sort()\n",
        "for ain in ains:\n",
        "    os.system('ffmpeg -y -loglevel error -i examples/{} -ar 16000 examples/tmp.wav'.format(ain))\n",
        "    if(not os.path.isfile('examples/tmp.wav')):\n",
        "        shutil.copyfile('examples/tmp.wav', 'examples/{}'.format(ain))\n",
        "\n",
        "    # au embedding\n",
        "    from thirdparty.resemblyer_util.speaker_emb import get_spk_emb\n",
        "    me, ae = get_spk_emb('examples/{}'.format(ain))\n",
        "    au_emb.append(me.reshape(-1))\n",
        "\n",
        "    print('Processing audio file', ain)\n",
        "    c = AutoVC_mel_Convertor('examples')\n",
        "    au_data_i = c.convert_single_wav_to_autovc_input(audio_filename=os.path.join('examples', ain),\n",
        "           autovc_model_path=opt_parser.load_AUTOVC_name)\n",
        "    au_data += au_data_i\n",
        "    # os.remove(os.path.join('examples', 'tmp.wav'))\n",
        "if(os.path.isfile('examples/tmp.wav')):\n",
        "    os.remove('examples/tmp.wav')\n",
        "\n",
        "fl_data = []\n",
        "rot_tran, rot_quat, anchor_t_shape = [], [], []\n",
        "for au, info in au_data:\n",
        "    au_length = au.shape[0]\n",
        "    fl = np.zeros(shape=(au_length, 68 * 3))\n",
        "    fl_data.append((fl, info))\n",
        "    rot_tran.append(np.zeros(shape=(au_length, 3, 4)))\n",
        "    rot_quat.append(np.zeros(shape=(au_length, 4)))\n",
        "    anchor_t_shape.append(np.zeros(shape=(au_length, 68 * 3)))\n",
        "\n",
        "if(os.path.exists(os.path.join('examples', 'dump', 'random_val_fl.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_fl.pickle'))\n",
        "if(os.path.exists(os.path.join('examples', 'dump', 'random_val_fl_interp.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_fl_interp.pickle'))\n",
        "if(os.path.exists(os.path.join('examples', 'dump', 'random_val_au.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_au.pickle'))\n",
        "if (os.path.exists(os.path.join('examples', 'dump', 'random_val_gaze.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_gaze.pickle'))\n",
        "\n",
        "with open(os.path.join('examples', 'dump', 'random_val_fl.pickle'), 'wb') as fp:\n",
        "    pickle.dump(fl_data, fp)\n",
        "with open(os.path.join('examples', 'dump', 'random_val_au.pickle'), 'wb') as fp:\n",
        "    pickle.dump(au_data, fp)\n",
        "with open(os.path.join('examples', 'dump', 'random_val_gaze.pickle'), 'wb') as fp:\n",
        "    gaze = {'rot_trans':rot_tran, 'rot_quat':rot_quat, 'anchor_t_shape':anchor_t_shape}\n",
        "    pickle.dump(gaze, fp)\n",
        "\n",
        "\n",
        "''' STEP 4: RUN audio->landmark network'''\n",
        "from src.approaches.train_audio2landmark import Audio2landmark_model\n",
        "model = Audio2landmark_model(opt_parser, jpg_shape=shape_3d)\n",
        "if(len(opt_parser.reuse_train_emb_list) == 0):\n",
        "    model.test(au_emb=au_emb)\n",
        "else:\n",
        "    model.test(au_emb=None)\n",
        "print('finish gen fls')\n",
        "\n",
        "''' STEP 5: de-normalize the output to the original image scale '''\n",
        "fls_names = glob.glob1(opt_parser.output_folder, 'pred_fls_*.txt')\n",
        "fls_names.sort()\n",
        "\n",
        "ains = glob.glob1('examples', '*.wav')\n",
        "ains = [item for item in ains if item is not 'tmp.wav']\n",
        "ains.sort()\n",
        "\n",
        "for i in range(0,len(fls_names)):\n",
        "    %cd /content/MakeItTalk\n",
        "    if (not ains or i >= len(ains)):\n",
        "        continue\n",
        "    ain = ains[i]\n",
        "    fl = np.loadtxt(os.path.join(opt_parser.output_folder, fls_names[i])).reshape((-1, 68,3))\n",
        "    output_dir = os.path.join(opt_parser.output_folder, fls_names[i][:-4])\n",
        "    try:\n",
        "        os.makedirs(output_dir)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    from util.utils import get_puppet_info\n",
        "\n",
        "    bound, scale, shift = get_puppet_info(DEMO_CH, ROOT_DIR=opt_parser.output_folder)\n",
        "\n",
        "    fls = fl.reshape((-1, 68, 3))\n",
        "\n",
        "    fls[:, :, 0:2] = -fls[:, :, 0:2]\n",
        "    fls[:, :, 0:2] = (fls[:, :, 0:2] / scale)\n",
        "    fls[:, :, 0:2] -= shift.reshape(1, 2)\n",
        "\n",
        "    fls = fls.reshape(-1, 204)\n",
        "\n",
        "    # additional smooth\n",
        "    from scipy.signal import savgol_filter\n",
        "    fls[:, 0:48*3] = savgol_filter(fls[:, 0:48*3], 17, 3, axis=0)\n",
        "    fls[:, 48*3:] = savgol_filter(fls[:, 48*3:], 11, 3, axis=0)\n",
        "    fls = fls.reshape((-1, 68, 3))\n",
        "\n",
        "    if (DEMO_CH in ['paint', 'mulaney', 'cartoonM', 'beer', 'color', 'JohnMulaney', 'vangogh', 'jm', 'roy', 'lineface']):\n",
        "        r = list(range(0, 68))\n",
        "        fls = fls[:, r, :]\n",
        "        fls = fls[:, :, 0:2].reshape(-1, 68 * 2)\n",
        "        fls = np.concatenate((fls, np.tile(bound, (fls.shape[0], 1))), axis=1)\n",
        "        fls = fls.reshape(-1, 160)\n",
        "\n",
        "    else:\n",
        "        r = list(range(0, 48)) + list(range(60, 68))\n",
        "        fls = fls[:, r, :]\n",
        "        fls = fls[:, :, 0:2].reshape(-1, 56 * 2)\n",
        "        fls = np.concatenate((fls, np.tile(bound, (fls.shape[0], 1))), axis=1)\n",
        "        fls = fls.reshape(-1, 112 + bound.shape[1])\n",
        "\n",
        "    np.savetxt(os.path.join(output_dir, 'warped_points.txt'), fls, fmt='%.2f')\n",
        "\n",
        "    # static_points.txt\n",
        "    static_frame = np.loadtxt(os.path.join(opt_parser.output_folder, '{}_face_open_mouth.txt'.format(DEMO_CH)))\n",
        "    static_frame = static_frame[r, 0:2]\n",
        "    static_frame = np.concatenate((static_frame, bound.reshape(-1, 2)), axis=0)\n",
        "    np.savetxt(os.path.join(output_dir, 'reference_points.txt'), static_frame, fmt='%.2f')\n",
        "\n",
        "    # triangle_vtx_index.txt\n",
        "    shutil.copy(os.path.join(opt_parser.output_folder, DEMO_CH + '_delauney_tri.txt'),\n",
        "                os.path.join(output_dir, 'triangulation.txt'))\n",
        "\n",
        "    os.remove(os.path.join(opt_parser.output_folder, fls_names[i]))\n",
        "\n",
        "    # ==============================================\n",
        "    # Step 4 : Vector art morphing\n",
        "    # ==============================================\n",
        "    warp_exe = os.path.join(os.getcwd(), 'facewarp', 'facewarp.exe')\n",
        "    import os\n",
        "    \n",
        "    if (os.path.exists(os.path.join(output_dir, 'output'))):\n",
        "        shutil.rmtree(os.path.join(output_dir, 'output'))\n",
        "    os.mkdir(os.path.join(output_dir, 'output'))\n",
        "    os.chdir('{}'.format(os.path.join(output_dir, 'output')))\n",
        "    cur_dir = os.getcwd()\n",
        "    \n",
        "    if(os.name == 'nt'): \n",
        "        ''' windows '''\n",
        "        os.system('{} {} {} {} {} {}'.format(\n",
        "            warp_exe,\n",
        "            os.path.join(cur_dir, '..', '..', opt_parser.jpg),\n",
        "            os.path.join(cur_dir, '..', 'triangulation.txt'),\n",
        "            os.path.join(cur_dir, '..', 'reference_points.txt'),\n",
        "            os.path.join(cur_dir, '..', 'warped_points.txt'),\n",
        "            os.path.join(cur_dir, '..', '..', opt_parser.jpg_bg),\n",
        "            '-novsync -dump'))\n",
        "    else:\n",
        "        ''' linux '''\n",
        "        os.system('wine {} {} {} {} {} {}'.format(\n",
        "            warp_exe,\n",
        "            os.path.join(cur_dir, '..', '..', opt_parser.jpg),\n",
        "            os.path.join(cur_dir, '..', 'triangulation.txt'),\n",
        "            os.path.join(cur_dir, '..', 'reference_points.txt'),\n",
        "            os.path.join(cur_dir, '..', 'warped_points.txt'),\n",
        "            os.path.join(cur_dir, '..', '..', opt_parser.jpg_bg),\n",
        "            '-novsync -dump'))\n",
        "    os.system('ffmpeg -y -r 62.5 -f image2 -i \"%06d.tga\" -i {} -pix_fmt yuv420p -vf \"pad=ceil(iw/2)*2:ceil(ih/2)*2\" -shortest -strict -2 {}'.format(\n",
        "        os.path.join(cur_dir, '..', '..', '..', 'examples', ain),\n",
        "        os.path.join(cur_dir, 'out.mp4')\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApCZszX-CvuP"
      },
      "source": [
        "## Visualize your animation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFBozkB1Cf8g",
        "outputId": "525045a3-069b-4600-f437-4ca669eb70d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "for ain in ains:\n",
        "  OUTPUT_MP4_NAME = '{}_pred_fls_{}_audio_embed.mp4'.format(\n",
        "    opt_parser.jpg.split('.')[0],\n",
        "    ain.split('.')[0]\n",
        "    )\n",
        "  mp4 = open('examples/{}'.format(OUTPUT_MP4_NAME),'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "  print('Display animation: examples/{}'.format(OUTPUT_MP4_NAME), file=sys.stderr)\n",
        "  display(HTML(\"\"\"\n",
        "  <video width=600 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % data_url))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0etBPyAC1e7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}